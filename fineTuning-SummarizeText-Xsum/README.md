# Fine-Tuning Phi-2 for Text Summarization (XSum)

This repository contains an **end-to-end fine-tuning pipeline**
for **abstractive text summarization** using **Phi-2** on the **XSum dataset**.
The project is optimized for **low VRAM environments** using:
- 4-bit quantization
- LoRA (Parameter-Efficient Fine-Tuning)

---

## ðŸš€ Features
- Low-resource fine-tuning (â‰¤ 8GB VRAM)
- Hugging Face Datasets & Transformers
- LoRA-based training
- ROUGE evaluation
- Training & evaluation visualization
- Ready-to-use inference pipeline

---

## ðŸ“‚ Repository Structure

